{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "latex_metadata": {
      "title": "Lesson 19. Solving Stochastic Dynamic Programs with Python"
    },
    "colab": {
      "name": "15 Solving stochastic dynamic programs with Python.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "hide_cell": true,
        "id": "04ZdNo5HDhNw",
        "colab_type": "text"
      },
      "source": [
        "<h5 class='prehead'>SA367 &middot; Mathematical Models for Decision Making &middot; Spring 2020 &middot; Uhan</h5>\n",
        "\n",
        "<h5 class='lesson'>Lesson 15.</h5>\n",
        "\n",
        "<h1 class='lesson_title'>Solving stochastic dynamic programs with Python</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PofjNf3DDhN4",
        "colab_type": "text"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEm5ERAMDhN5",
        "colab_type": "text"
      },
      "source": [
        "* Let's solve the stochastic dynamic program we formulated for the investment problem in Lesson 14.\n",
        "\n",
        "* In this class, we will use a package called `stochasticdp` to set up and solve stochastic dynamic programs.\n",
        "    - _Warning._ This is a package that I wrote. There may be some bugs.\n",
        "    - _Note._ This package is publicly available. Please feel free to use it in the future for other things. The source code is on [GitHub](https://github.com/nelsonuhan/stochasticdp)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2Ov-Q0kDhN7",
        "colab_type": "text"
      },
      "source": [
        "## Installing `stochasticdp`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb-CzRATDhN9",
        "colab_type": "text"
      },
      "source": [
        "* If you're using a local installation of Python (e.g. Anaconda), open a prompt and type:\n",
        "\n",
        "    ```\n",
        "    pip install stochasticdp\n",
        "    ```\n",
        "\n",
        "* If you're running this on Google Colaboratory, then just run the cell below:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "028LzBuaEUtl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "54d7a54a-43ad-44e8-eb1c-8f4b6c619477"
      },
      "source": [
        "!pip install stochasticdp"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: stochasticdp in /usr/local/lib/python3.6/dist-packages (0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlHh3zaUER5j",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "* To use `stochasticdp`, we must first import it. In `stochasticdp`, we only need the object `StochasticDP`, so we can perform our import like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WRCohCRDhN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from stochasticdp import StochasticDP"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_vfIyFHDhOG",
        "colab_type": "text"
      },
      "source": [
        "## Setting up a stochastic dynamic program"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7H9jvwEDhOG",
        "colab_type": "text"
      },
      "source": [
        "* Recall the investment problem from Example 2 in Lesson 14:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiyPxt6gDhOH",
        "colab_type": "text"
      },
      "source": [
        "__Example 2 in Lesson 14.__ Suppose you have \\$5,000 to invest. Over the next 3 years, you want to double your money.\n",
        "At the beginning of each of the next 3 years, you have an opportunity to invest in one of two investments: A or B. Both investments have uncertain profits. For an investment of \\$5,000, the profits are as follows:\n",
        "\n",
        "\n",
        "| Investment | Profit (\\$) | Probability |\n",
        "|:-----------|------------:|------------:|\n",
        "| A          | -5,000      | 0.3         |\n",
        "|            | 5,000       | 0.7         |\n",
        "| B          | 0           | 0.9         |\n",
        "|            | 5,000       | 0.1         |\n",
        "                                     \n",
        "You are allowed to make at most one investment each year, and can invest only \\\\$5,000 each time. Any additional money accumulated is left idle. Once you've accumulated \\\\$10,000, you stop investing.\n",
        "\n",
        "Formulate a stochastic dynamic program to find an investment policy that maximizes the probability you will have \\$10,000 after 3 years."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQocY-m2DhOH",
        "colab_type": "text"
      },
      "source": [
        "* Let's walk through setting up the stochastic DP we formulated for this problem in Lesson 14."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRTmu5-MDhOI",
        "colab_type": "text"
      },
      "source": [
        "### Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7F2kAPFDhOJ",
        "colab_type": "text"
      },
      "source": [
        "* We had defined 4 stages.\n",
        "\n",
        "* To make things easier, let's renumber the stages so they start at $t = 0$:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\text{stage } t = 0, 1, 2 & \\quad\\leftrightarrow\\quad \\text{beginning of year $t$}\\\\\n",
        "t = 3 & \\quad\\leftrightarrow\\quad \\text{end of process}\n",
        "\\end{aligned}\n",
        "$$\n",
        "    \n",
        "* In each stage, we defined 3 states:\n",
        "\n",
        "$$\n",
        "\\text{state } n \\in \\{0, 5000, 10000\\} \\quad\\leftrightarrow\\quad \\text{$n$ dollars in account}\n",
        "$$\n",
        "\n",
        "* At each stage and state, we defined 3 possible decisions:\n",
        "\n",
        "$$\n",
        "\\text{decision } x_t \\in \\{ \\text{A}, \\text{B}, \\text{no investment} \\}\n",
        "$$\n",
        "\n",
        "* The set of _allowable_ decisions changes, depending on the stage and state. We'll address this later.\n",
        "\n",
        "* For now, we can initialize a stochastic dynamic program with these stages, states, and decisions like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVMASEJJDhOJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Number of stages\n",
        "number_of_stages = 4\n",
        "\n",
        "# List of states\n",
        "states = [0, 5000, 10000]\n",
        "\n",
        "# List of decisions\n",
        "decisions = ['A', 'B', 'no investment']\n",
        "\n",
        "# Initialize stochastic dynamic program - we want to maximize, so minimize = False\n",
        "dp = StochasticDP(number_of_stages, states, decisions, minimize=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H20UXiANDhOM",
        "colab_type": "text"
      },
      "source": [
        "* The code above initializes a stochastic dynamic program called `dp`.\n",
        "\n",
        "* Next, we need to add every transition that occurs with positive probability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbRHRe_sDhON",
        "colab_type": "text"
      },
      "source": [
        "### Transition probabilities and contributions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxJXSJnXDhOV",
        "colab_type": "text"
      },
      "source": [
        "* First, let's tackle transitions from the state $n = 5000$:\n",
        "\n",
        "<center><img src=\"https://github.com/sa367-uhan/stochastic-dp-python/blob/colab-master/img/5000.png?raw=1\" width=\"60%\"></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5elABJ5DhOW",
        "colab_type": "text"
      },
      "source": [
        "* We can add a transition from state $n$ to state $m$ in stage $t$ under decision $x$ with probability $p(m \\,|\\, n, t, x)$ and contribution $c(m \\,|\\, n, t, x)$ as follows:\n",
        "\n",
        "    ```python\n",
        "    dp.add_transition(stage=t, from_state=n, decision=x, to_state=m, probability=p, contribution=c)\n",
        "    ```\n",
        "\n",
        "* So, we can input the transition probabilities and contributions from state $n = 5000$ in stages $t = 0, 1, 2$ as follows.\n",
        "\n",
        "    * Remember that the contributions for all transitions are 0 in this stochastic DP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdAwRfLGDhOX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transition probabilities and contributions from state n = 5000\n",
        "for t in range(number_of_stages - 1):\n",
        "    # Investment A\n",
        "    dp.add_transition(stage=t, from_state=5000, decision='A', to_state=10000, probability=0.7, contribution=0)\n",
        "    dp.add_transition(stage=t, from_state=5000, decision='A', to_state=0, probability=0.3, contribution=0)\n",
        "\n",
        "    # Investment B\n",
        "    dp.add_transition(stage=t, from_state=5000, decision='B', to_state=10000, probability=0.1, contribution=0)\n",
        "    dp.add_transition(stage=t, from_state=5000, decision='B', to_state=5000, probability=0.9, contribution=0)\n",
        "\n",
        "    # No investment\n",
        "    dp.add_transition(stage=t, from_state=5000, decision='no investment', to_state=5000, probability=1, contribution = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovuQRaZPDhOc",
        "colab_type": "text"
      },
      "source": [
        "* _Quick check._ What can the sum of the transition probabilities from any decision node equal?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxjGeE9EDhOe",
        "colab_type": "text"
      },
      "source": [
        "_Write your notes here. Double-click to edit._\n",
        "\n",
        "The transition probabilities from any decision node must add up to either 0 or 1. They will add up to 1 if the decision is allowable at that stage/state; otherwise they will add up to 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEmReO9UDhOe",
        "colab_type": "text"
      },
      "source": [
        "* Next, let's tackle the transitions from state $n = 0$:\n",
        "\n",
        "<center><img src=\"https://github.com/sa367-uhan/stochastic-dp-python/blob/colab-master/img/0.png?raw=1\"/></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10as-rQXDhOf",
        "colab_type": "text"
      },
      "source": [
        "* So, we can input the transition probabilities and contributions from state $n = 0$ in stages $t = 0, 1, 2$ like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBUsBlhNDhOf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transition probabilities and contributions from state n = 0\n",
        "for t in range(number_of_stages - 1):\n",
        "    # No investment\n",
        "    dp.add_transition(stage=t, from_state=0, decision='no investment', to_state=0, probability=1, contribution=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oE5E32kDhOj",
        "colab_type": "text"
      },
      "source": [
        "* We can tackle the transitions from state $n = 10000$ in an almost identical way:\n",
        "\n",
        "<center><img src=\"https://github.com/sa367-uhan/stochastic-dp-python/blob/colab-master/img/10000.png?raw=1\" /></center>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXp6lEuPDhOj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transition probabilities and contributions from state n = 10000\n",
        "for t in range(number_of_stages - 1):\n",
        "    # No investment\n",
        "    dp.add_transition(stage=t, from_state=10000, decision='no investment', to_state=10000, probability=1, contribution=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtQk23nODhOp",
        "colab_type": "text"
      },
      "source": [
        "### Boundary conditions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSODkVlhDhOq",
        "colab_type": "text"
      },
      "source": [
        "* Finally, we need to define the boundary conditions.\n",
        "\n",
        "* In particular, we need to specify the value-to-go function at the last stage (in our case, $t = 3$) for each state.\n",
        "\n",
        "* We can add the boundary conditions for state $n$ like this:\n",
        "\n",
        "    ```python\n",
        "    dp.add_boundary(state=0, value=0)\n",
        "    ```\n",
        "\n",
        "* So, let's add the boundary conditions for our problem:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBD-hKiwDhOq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Boundary conditions\n",
        "dp.add_boundary(state=10000, value=1)\n",
        "dp.add_boundary(state=5000, value=0)\n",
        "dp.add_boundary(state=0, value=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9zQUS34DhOs",
        "colab_type": "text"
      },
      "source": [
        "## Solving the stochastic dynamic program"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUNyTC4zDhOt",
        "colab_type": "text"
      },
      "source": [
        "* Once the stochastic DP is setup, we can solve it like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VthKx4qdDhOt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Solve the stochastic dynamic program\n",
        "value, policy = dp.solve()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5sIdsLtDhOw",
        "colab_type": "text"
      },
      "source": [
        "* Note that the method `.solve()` outputs two objects: `value` and `policy`.\n",
        "\n",
        "* `value[t, n]` is the value-to-go function $f_t(n)$ at stage $t$ and state $n$.\n",
        "\n",
        "* `policy[t, n]` is the optimal decision $x_t^*$ that attains the value-to-go function $f_t(n)$ at stage $t$ and state $n$.\n",
        "\n",
        "* First, let's see what the value-to-go function looks like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9aYaxjnDhOx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "e3a5273b-4e85-4208-c3f7-ab172699fd55"
      },
      "source": [
        "# Examine the value-to-go function\n",
        "print(value)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{(stage: 0, state: 0): 0\n",
            " (stage: 0, state: 5000): 0.757\n",
            " (stage: 0, state: 10000): 1\n",
            " (stage: 1, state: 0): 0\n",
            " (stage: 1, state: 5000): 0.73\n",
            " (stage: 1, state: 10000): 1\n",
            " (stage: 2, state: 0): 0\n",
            " (stage: 2, state: 5000): 0.7\n",
            " (stage: 2, state: 10000): 1\n",
            " (stage: 3, state: 0): 0\n",
            " (stage: 3, state: 5000): 0\n",
            " (stage: 3, state: 10000): 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfQJ_DveDhO5",
        "colab_type": "text"
      },
      "source": [
        "* Next, let's look at the corresponding policy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "au8dyT9nDhO5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "7a9c51ac-d117-45a2-d5b8-ef41e6d6e7e1"
      },
      "source": [
        "# Examine the policy\n",
        "print(policy)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{(stage: 0, state: 0): {'no investment'}\n",
            " (stage: 0, state: 5000): {'B'}\n",
            " (stage: 0, state: 10000): {'no investment'}\n",
            " (stage: 1, state: 0): {'no investment'}\n",
            " (stage: 1, state: 5000): {'B'}\n",
            " (stage: 1, state: 10000): {'no investment'}\n",
            " (stage: 2, state: 0): {'no investment'}\n",
            " (stage: 2, state: 5000): {'A'}\n",
            " (stage: 2, state: 10000): {'no investment'}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfFNI-M5DhO8",
        "colab_type": "text"
      },
      "source": [
        "## On your own"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpeblVzvDhO8",
        "colab_type": "text"
      },
      "source": [
        "* Here is Example 1 from Lesson 14. Set up and solve the stochastic DP we formulated for this problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3qASyBDDhO8",
        "colab_type": "text"
      },
      "source": [
        "__Example 1 from Lesson 14.__ The Hit-and-Miss Manufacturing Company has received an order to supply one item of a particular type. However, manufacturing this item is difficult, and the customer has specified such stringent quality requirements that the company may have to produce more than one item to obtain an item that is acceptable.\n",
        "                \n",
        "The company estimates that each item of this type will be acceptable with probability 1/2 and defective with probability 1/2. Each item costs \\$100 to produce, and excess items are worthless. In addition, a setup cost of \\$300 must be incurred whenever the production process is setup for this item. The company has time to make no more than 3 production runs, and at most 5 items can be produced in each run. If an acceptable item has not been obtained by the end of the third production run, the manufacturer is in breach of contract and must pay a penalty of \\$1600.\n",
        "                \n",
        "The objective is to determine how many items to produce in each production run in order to minimize the total expected cost."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "EJVK88n7DhO9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "c6e3d0a1-f569-4252-a718-6f4eaff535f8"
      },
      "source": [
        "# Number of stages\n",
        "number_of_stages = 4\n",
        "\n",
        "# List of states\n",
        "states = [0, 1]\n",
        "\n",
        "# List of decisions\n",
        "decisions = [0, 1, 2, 3, 4, 5]\n",
        "\n",
        "# Initialize stochastic dynamic program\n",
        "dp = StochasticDP(number_of_stages, states, decisions, minimize=True)\n",
        "\n",
        "# Transition probabilities and contributions from state n = 0\n",
        "for t in range(number_of_stages - 1):\n",
        "    for x in decisions:\n",
        "        if x > 0:\n",
        "            K = 300\n",
        "        else:\n",
        "            K = 0\n",
        "            \n",
        "        dp.add_transition(stage=t, from_state=0, decision=x, to_state=0, probability=1, contribution=K + 100 * x)\n",
        "        dp.add_transition(stage=t, from_state=0, decision=x, to_state=1, probability=0, contribution=K + 100 * x)        \n",
        "\n",
        "# Transition probabilities and contributions from state n = 1\n",
        "for t in range(number_of_stages - 1):\n",
        "    for x in decisions:\n",
        "        if x > 0:\n",
        "            K = 300\n",
        "        else:\n",
        "            K = 0\n",
        "            \n",
        "        dp.add_transition(stage=t, from_state=1, decision=x, to_state=0, probability=1 - (1/2)**x, contribution=K + 100 * x)\n",
        "        dp.add_transition(stage=t, from_state=1, decision=x, to_state=1, probability=(1/2)**x, contribution=K + 100 * x)\n",
        "        \n",
        "# Boundary conditions\n",
        "dp.add_boundary(state=0, value=0)\n",
        "dp.add_boundary(state=1, value=1600)\n",
        "\n",
        "# Solve the stochastic dynamic program\n",
        "value, policy = dp.solve()\n",
        "\n",
        "# Examine value-to-go\n",
        "print(\"The value-to-go function is:\")\n",
        "print(value)\n",
        "\n",
        "# Examine policy\n",
        "print(\"\\nThe corresponding policy is:\")\n",
        "print(policy)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The value-to-go function is:\n",
            "{(stage: 0, state: 0): 0\n",
            " (stage: 0, state: 1): 675.0\n",
            " (stage: 1, state: 0): 0\n",
            " (stage: 1, state: 1): 700.0\n",
            " (stage: 2, state: 0): 0\n",
            " (stage: 2, state: 1): 800.0\n",
            " (stage: 3, state: 0): 0\n",
            " (stage: 3, state: 1): 1600}\n",
            "\n",
            "The corresponding policy is:\n",
            "{(stage: 0, state: 0): {0}\n",
            " (stage: 0, state: 1): {2}\n",
            " (stage: 1, state: 0): {0}\n",
            " (stage: 1, state: 1): {2, 3}\n",
            " (stage: 2, state: 0): {0}\n",
            " (stage: 2, state: 1): {3, 4}}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}